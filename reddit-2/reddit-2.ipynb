{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP\n",
    "\n",
    "In this notebook, I will review some of the most fundamental ideas in Natural Language Processing (NLP). We will start with a very naive kind of sentiment analysis and this will force us to also consider some of the most important techniques for preprocessing natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "One of the most popular kinds of NLP analyses used extensively in computational social science is sentiment analysis. The notion of sentiment refers to the emotional valence of a given text or utterance. Valence is typically defined in this context as a one-dimensional and bipolar construct and refers to the extent to which something is infused with negative or positive emotions/attitudes. However, there are also more elaborate kinds of sentiment analyses that define sentiment in terms of multiple dimensions usually referring to some model of basic (primitive) emotions (i.e. anger, disgust, fear, happiness, sadness, and surprise).\n",
    "\n",
    "Sentiment analysis is often used in analyses of social media, speeches of public persons, press and many as well as in many other contexts.\n",
    "\n",
    "In technical terms sentiment analysis may be conducted in many different ways (most sophisticated approaches are based on complex deep learning models). Nonetheless, all the methods are always based on some prior linguistic datasets (usually called lexicons or corpora) that assign some scores, representing emotional valences, to given words or phrases.\n",
    "\n",
    "Here I will not bore you with the most basic and naive (and frankly not very useful) sentiment analysis based on the so-called [AFINN](http://corpustext.com/reference/sentiment_afinn.html) lexicon. The lexicon assigns sentiment scores ranging from -5 (very negative) to +5 (very positive) to individual English words (and it includes only about 2500 of them). It computes sentiment in a very simple way by just matching individual words from a larger text with their scores in the AFINN lexicon and computing different kinds of average scores based on that.\n",
    "\n",
    "Instead, I will use a more nuanced type of sentiment tailored for web data (such as blog posts, etc.) called [VADER](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8109), which is implemented in a powerful Python package for NLP called [NLTK](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "One of the biggest difficulties of NLP analysis stems from the fact that natural language is very contextual and messy. Many words may mean different things depending on context or might be spelled or written differently, for instance, depending on their position in a sentence while still being semantically equivalent. Other words may not have any intrinsic meaning as they play an only grammatical role. A good example of that are articles in English (i.e. a, an, the).\n",
    "\n",
    "### Stop words\n",
    "\n",
    "In our simple sentiment analysis, we will be concerned mostly with average sentiment scores over all words in a given text. Therefore, one of our concerns will be to first get rid of words with no clear semantics such as articles. Usually, such words in the context of NLP are called **stop words**. So we will want to get rid of all the stop words from our texts because they would bias our sentiment scores downwards.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "However, first, we have to notice that our approach will be based on the analysis of individual words. And initially, our texts will be single strings. Thus, first, we will have to decompose texts into single words. Such a process is usually called **tokenization** and it refers to a decomposition of a text into lower-order elements such as words or sentences. The naive way to do that would be to split a text by any kind of whitespace, but in practice, this is too simplistic. Luckily, people already studied this problem quite extensively and figured out better solutions, so we will not have to reinvent the wheel. Instead, we will use one of the methods offered by the NLTK package.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "After tokenization, we would be able to remove stop words and lookup sentiment scores for the rest of the words. However, there is still some problem that we should address before that. Many words with the same meaning may be written differently in different contexts, for instance, depending on whether they occur in singular or plural form or depending on tense, etc.\n",
    "\n",
    "One way to deal with that is to convert words to their lemmas in the process called **lemmatization**. A lemma of a word is its core form. Below we provide sum examples:\n",
    "\n",
    "* houses $\\rightarrow$ house\n",
    "* are $\\rightarrow$ is\n",
    "* mice $\\rightarrow$ mouse\n",
    "* becoming $\\rightarrow$ become\n",
    "\n",
    "Very accurate lemmatization is possible, but in general, it is rather hard and requires some additional work to be done that is beyond the scope of this course. Here we will do only a very simple kind of lemmatization that will allow us to simplify all plural forms into singular forms.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "Summing up, the data processing pipeline that we will use here will be the following:\n",
    "\n",
    "$$\\text{text} \\rightarrow \\text{tokenization} \\rightarrow \\text{lemmatization} \\rightarrow \\text{stop words removal} \\rightarrow \\text{sentiment analysis}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit (NLTK)\n",
    "\n",
    "NLTK is one of the most important and popular Python packages for Natural Language Processing. It is a very powerful but also complex package and we will not discuss any details of how it works. Instead, we will only use a few tools it provides. However, what we will show is enough to conduct simple sentiment analysis. Thus, the techniques presented here will constitute the last element that together with the things we learned previously will allow you to conduct a simple computational study starting with data extraction and ending with simple natural language analysis.\n",
    "\n",
    "As we already mentioned, one of the characteristic features of NLP is that it is usually based (one way or another) on some preexisting datasets called lexicons and/or corpora compile by linguists and other people who study natural languages. As a result, quite often working with NLTK starts with downloading some additional datasets that will be needed to perform particular analyses. Luckily, this is very easy with NLTK as it provides a very simple API for downloading missing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "Before we move to computing the sentiment we need some text to do so. Therefore, we will use the data you have already collected. We will use submissions because they have more text to process. \n",
    "\n",
    "**But how are we going to do that?**\n",
    "\n",
    "We will use exactly the same script as I put in the last part of the last notebook under the Hint. However, I will unpack in more details what happens here to give you a better understanding of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required module\n",
    "import json\n",
    "\n",
    "## Open the file in read mode\n",
    "with open('comments.jl', 'r') as file:\n",
    "    ## Read line by line and convert every line into a dict\n",
    "    ## Store everything in a list\n",
    "    data = [json.loads(line) for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ilustration how VADER works we will need some text. First we will foucus on one comment only and later I will show you how to do it for all comments. So let's extract the longest comment from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the empty string\n",
    "longest_comment = \"\"\n",
    "\n",
    "## Loop over the list\n",
    "for line in data:\n",
    "    ## Pick only a string longer than the string before\n",
    "    if len(line['body'])>len(longest_comment):\n",
    "        ## Assign the string to the longest_comment object\n",
    "        longest_comment = line['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment with VADER\n",
    "\n",
    "First, I thought about showing you also very naive way of computing sentiment - dictionary based but afterward I realised that there was no point in that. It was enough I told you about it cause it is more or less intuittive what happens.\n",
    "\n",
    "VADER is a slightly more complex approach that takes into account issues such as exclamation marks, negations and adjectival modifiers (i.e. words such as \"very\"). Its implementation is much more complex than what we did previously and we will not discuss it here. The good thing is that it is implemented in NLTK and extremely easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mikolaj/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import VADER, download its lexicon and initialize an instance of a sentiment analyzer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Import sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment1 = vader.polarity_scores(longest_comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.073, 'neu': 0.84, 'pos': 0.087, 'compound': 0.8432}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.107, 'neu': 0.796, 'pos': 0.097, 'compound': -0.0772},\n",
       " {'neg': 0.123, 'neu': 0.764, 'pos': 0.113, 'compound': -0.431}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment of sentences (more fine-grained picture)\n",
    "sent_sentiment1 = [ vader.polarity_scores(sent) for sent in sent_tokenize(longest_comment) ]\n",
    "sent_sentiment1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average compound score over sentences (text I): 0.0033395061728394987\n"
     ]
    }
   ],
   "source": [
    "# For instance, we can look at average compound scores over sentences\n",
    "sent_sentiment1_compound = sum(s['compound'] for s in sent_sentiment1) / len(sent_sentiment1)\n",
    "print(\"Average compound score over sentences (text I):\", sent_sentiment1_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data[:10]:\n",
    "    line['neg'] = vader.polarity_scores(line['body'])['neg']\n",
    "    line['pos'] = vader.polarity_scores(line['body'])['pos']\n",
    "    line['neg'] = vader.polarity_scores(line['body'])['neu']\n",
    "    line['neg'] = vader.polarity_scores(line['body'])['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author': 'rcglinsk',\n",
       "  'author_created_utc': 1205768861,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'author_fullname': 't2_33y7d',\n",
       "  'body': 'The bounty hunter responds to Jabba, \"400 ppm no less\"',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-14 04:08:12',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0402wl',\n",
       "  'link_id': 't3_6j98u',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6j98u',\n",
       "  'reply_delay': 23467,\n",
       "  'retrieved_on': 1425849771,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.296,\n",
       "  'pos': 0.0},\n",
       " {'author': 'oku',\n",
       "  'author_created_utc': 1159679760,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'author_fullname': 't2_kf08',\n",
       "  'body': 'I had problems with ftp, here is another link:\\nhttp://www1.ncdc.noaa.gov/pub/data/paleo/historical/switzerland/meier2007grape.txt',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-17 06:28:56',\n",
       "  'distinguished': None,\n",
       "  'edited': 1,\n",
       "  'gilded': 0,\n",
       "  'id': 'c041i5q',\n",
       "  'link_id': 't3_6jeme',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6jeme',\n",
       "  'reply_delay': 202157,\n",
       "  'retrieved_on': 1425850593,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.4019,\n",
       "  'pos': 0.0},\n",
       " {'author': 'oku',\n",
       "  'author_created_utc': 1159679760,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'author_fullname': 't2_kf08',\n",
       "  'body': \"That website is painful for my browser...\\n\\nFor a thorough discussion, see for example [tamino's blog](http://tamino.wordpress.com/2007/07/24/pmod-vs-acrim/).\",\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-20 07:20:19',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c042ib9',\n",
       "  'link_id': 't3_6k03l',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6k03l',\n",
       "  'reply_delay': 27538,\n",
       "  'retrieved_on': 1425851361,\n",
       "  'score': 2,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.4404,\n",
       "  'pos': 0.0},\n",
       " {'author': '[deleted]',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'body': 'Fuck Hansen and fuck silence7, who started this subreddit by banning everyone who disagrees with him on the topic of climate change. GG you autocratic Nazi cocksucker.',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-30 02:28:25',\n",
       "  'distinguished': None,\n",
       "  'edited': 1,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0469ee',\n",
       "  'link_id': 't3_6lb98',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6lb98',\n",
       "  'reply_delay': 9963,\n",
       "  'retrieved_on': 1425854086,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.8877,\n",
       "  'pos': 0.077},\n",
       " {'author': '[deleted]',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'body': 'Fuck Hansen and fuck silence7, who started this subreddit by banning everyone who disagrees with him on the topic of climate change. GG you autocratic Nazi cocksucker.',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-30 02:32:01',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0469fy',\n",
       "  'link_id': 't3_6kop2',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6kop2',\n",
       "  'reply_delay': 419079,\n",
       "  'retrieved_on': 1425854086,\n",
       "  'score': 0,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.8877,\n",
       "  'pos': 0.077},\n",
       " {'author': '[deleted]',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'body': 'Fuck Hansen and fuck silence7, who started this subreddit by banning everyone who disagrees with him on the topic of climate change. GG you autocratic Nazi cocksucker.',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-30 02:32:14',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0469g4',\n",
       "  'link_id': 't3_6kji0',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6kji0',\n",
       "  'reply_delay': 545985,\n",
       "  'retrieved_on': 1425854086,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.8877,\n",
       "  'pos': 0.077},\n",
       " {'author': '[deleted]',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'body': 'Fuck Hansen and fuck silence7, who started this subreddit by banning everyone who disagrees with him on the topic of climate change. GG you autocratic Nazi cocksucker.',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-30 02:32:39',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0469gi',\n",
       "  'link_id': 't3_6kios',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6kios',\n",
       "  'reply_delay': 556455,\n",
       "  'retrieved_on': 1425854097,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.8877,\n",
       "  'pos': 0.077},\n",
       " {'author': '[deleted]',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'body': 'Fuck Hansen and fuck silence7, who started this subreddit by banning everyone who disagrees with him on the topic of climate change. GG you autocratic Nazi cocksucker.',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-30 02:32:48',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0469gj',\n",
       "  'link_id': 't3_6kisc',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6kisc',\n",
       "  'reply_delay': 555233,\n",
       "  'retrieved_on': 1425854097,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.8877,\n",
       "  'pos': 0.077},\n",
       " {'author': '[deleted]',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'body': 'Fuck Hansen and fuck silence7, who started this subreddit by banning everyone who disagrees with him on the topic of climate change. GG you autocratic Nazi cocksucker.',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-30 02:32:56',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0469gl',\n",
       "  'link_id': 't3_6kgf2',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6kgf2',\n",
       "  'reply_delay': 603139,\n",
       "  'retrieved_on': 1425854097,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.8877,\n",
       "  'pos': 0.077},\n",
       " {'author': '[deleted]',\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'body': 'Fuck Hansen and fuck silence7, who started this subreddit by banning everyone who disagrees with him on the topic of climate change. GG you autocratic Nazi cocksucker.',\n",
       "  'controversiality': 0,\n",
       "  'created_utc': '2008-05-30 02:33:03',\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'c0469gn',\n",
       "  'link_id': 't3_6k4ix',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_6k4ix',\n",
       "  'reply_delay': 799715,\n",
       "  'retrieved_on': 1425854097,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'climate',\n",
       "  'subreddit_id': 't5_2qhx3',\n",
       "  'neg': -0.8877,\n",
       "  'pos': 0.077}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
