{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP\n",
    "\n",
    "In this notebook, I will review some of the most fundamental ideas in Natural Language Processing (NLP). We will start with a very naive kind of sentiment analysis and this will force us to also consider some of the most important techniques for preprocessing natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "One of the most popular kinds of NLP analyses used extensively in computational social science is sentiment analysis. The notion of sentiment refers to the emotional valence of a given text or utterance. Valence is typically defined in this context as a one-dimensional and bipolar construct and refers to the extent to which something is infused with negative or positive emotions/attitudes. However, there are also more elaborate kinds of sentiment analyses that define sentiment in terms of multiple dimensions usually referring to some model of basic (primitive) emotions (i.e. anger, disgust, fear, happiness, sadness, and surprise).\n",
    "\n",
    "Sentiment analysis is often used in analyses of social media, speeches of public persons, press and many as well as in many other contexts.\n",
    "\n",
    "In technical terms sentiment analysis may be conducted in many different ways (most sophisticated approaches are based on complex deep learning models). Nonetheless, all the methods are always based on some prior linguistic datasets (usually called lexicons or corpora) that assign some scores, representing emotional valences, to given words or phrases.\n",
    "\n",
    "Here I will not bore you with the most basic and naive (and frankly not very useful) sentiment analysis based on the so-called [AFINN](http://corpustext.com/reference/sentiment_afinn.html) lexicon. The lexicon assigns sentiment scores ranging from -5 (very negative) to +5 (very positive) to individual English words (and it includes only about 2500 of them). It computes sentiment in a very simple way by just matching individual words from a larger text with their scores in the AFINN lexicon and computing different kinds of average scores based on that.\n",
    "\n",
    "Instead, I will use a more nuanced type of sentiment tailored for web data (such as blog posts, etc.) called [VADER](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8109), which is implemented in a powerful Python package for NLP called [NLTK](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "One of the biggest difficulties of NLP analysis stems from the fact that natural language is very contextual and messy. Many words may mean different things depending on context or might be spelled or written differently, for instance, depending on their position in a sentence while still being semantically equivalent. Other words may not have any intrinsic meaning as they play an only grammatical role. A good example of that are articles in English (i.e. a, an, the).\n",
    "\n",
    "### Stop words\n",
    "\n",
    "In our simple sentiment analysis, we will be concerned mostly with average sentiment scores over all words in a given text. Therefore, one of our concerns will be to first get rid of words with no clear semantics such as articles. Usually, such words in the context of NLP are called **stop words**. So we will want to get rid of all the stop words from our texts because they would bias our sentiment scores downwards.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "However, first, we have to notice that our approach will be based on the analysis of individual words. And initially, our texts will be single strings. Thus, first, we will have to decompose texts into single words. Such a process is usually called **tokenization** and it refers to a decomposition of a text into lower-order elements such as words or sentences. The naive way to do that would be to split a text by any kind of whitespace, but in practice, this is too simplistic. Luckily, people already studied this problem quite extensively and figured out better solutions, so we will not have to reinvent the wheel. Instead, we will use one of the methods offered by the NLTK package.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "After tokenization, we would be able to remove stop words and lookup sentiment scores for the rest of the words. However, there is still some problem that we should address before that. Many words with the same meaning may be written differently in different contexts, for instance, depending on whether they occur in singular or plural form or depending on tense, etc.\n",
    "\n",
    "One way to deal with that is to convert words to their lemmas in the process called **lemmatization**. A lemma of a word is its core form. Below we provide sum examples:\n",
    "\n",
    "* houses $\\rightarrow$ house\n",
    "* are $\\rightarrow$ is\n",
    "* mice $\\rightarrow$ mouse\n",
    "* becoming $\\rightarrow$ become\n",
    "\n",
    "Very accurate lemmatization is possible, but in general, it is rather hard and requires some additional work to be done that is beyond the scope of this course. Here we will do only a very simple kind of lemmatization that will allow us to simplify all plural forms into singular forms.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "Summing up, the data processing pipeline that we will use here will be the following:\n",
    "\n",
    "$$\\text{text} \\rightarrow \\text{tokenization} \\rightarrow \\text{lemmatization} \\rightarrow \\text{stop words removal} \\rightarrow \\text{sentiment analysis}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit (NLTK)\n",
    "\n",
    "NLTK is one of the most important and popular Python packages for Natural Language Processing. It is a very powerful but also complex package and we will not discuss any details of how it works. Instead, we will only use a few tools it provides. However, what we will show is enough to conduct simple sentiment analysis. Thus, the techniques presented here will constitute the last element that together with the things we learned previously will allow you to conduct a simple computational study starting with data extraction and ending with simple natural language analysis.\n",
    "\n",
    "As we already mentioned, one of the characteristic features of NLP is that it is usually based (one way or another) on some preexisting datasets called lexicons and/or corpora compile by linguists and other people who study natural languages. As a result, quite often working with NLTK starts with downloading some additional datasets that will be needed to perform particular analyses. Luckily, this is very easy with NLTK as it provides a very simple API for downloading missing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "Before we move to computing the sentiment we need some text to do so. Therefore, we will use the data you have already collected. We will use submissions because they have more text to process. \n",
    "\n",
    "**But how are we going to do that?**\n",
    "\n",
    "We will use exactly the same script as I put in the last part of the last notebook under the Hint. However, I will unpack in more details what happens here to give you a better understanding of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required module\n",
    "import json\n",
    "\n",
    "## Open the file in read mode\n",
    "with open('../scripts/comments.jl', 'r') as file:\n",
    "    ## Read line by line and convert every line into a dict\n",
    "    ## Store everything in a list\n",
    "    data = [json.loads(line) for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ilustration how VADER works we will need some text. First we will foucus on one comment only and later I will show you how to do it for all comments. So let's extract the longest comment from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_comment = \"\"\n",
    "for line in data:\n",
    "    if len(line['body'])>len(longest_comment):\n",
    "        longest_comment = line['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He draws a false equivalence between science and religion. Science is based on the scientific method. It works, and if it doesn\\'t it changes until it does.\\n\\nReligion is faith based, with the \"truths\" passed down from the past, and are not tested or changed if they are shown to be wrong.\\n\\nSo now that we know that we can use science and that we should use science lets look at his actual arguments:\\n\\n&gt; We sense a growing number of skeptics who accept that the Earth may be warming, but question whether nature is the driving force rather than a vain attempt by man to accept blame as a form of industrialization penance\\n\\nWe may sense this, but we would be sensing uninformed opinions.\\n\\nThe scientific community is pretty much 97-3 on the side of \"most of the warming since the middle of last century is likely to be anthropogenic\".\\n\\nLook perhaps at the wiki page on [the scientific opinion on climate change](http://en.wikipedia.org/wiki/Scientific_opinion_on_climate_change). The concurring scientific organizations include all basically the national science academies of the world, and the Physics/Chemistry, paleontology, and biology scientific organizations. Also the vast majority of meteorology, earth science, and other scientific organizations.\\n\\nNon-committal organizations number 5, including 4 in which the majority of the membership are professional geologists.\\n\\nDissenting scientific organizations number zero. There are none. *\"As of 2007, when the American Association of Petroleum Geologists released a revised statement, no scientific body of national or international standing rejected the findings of human-induced effects on climate change.\"*\\n\\n&gt; In their book Unstoppable Global Warming – Every 1,500 Years authors S. Fred Singer and Dennis T. Avery describe climate cycles in natural terms.\\n\\n\"The 1500-year cycle in question has been observed mainly through ice core data as a warming in the northern hemisphere matched at precisely the same time by a cooling in the southern hemisphere. So it’s a heat distribution issue:  a global temperature ‘see-saw’ effect. The total heat in the global system remains constant.\"\\n\\n\"In contrast, human-produced global warming has been caused by the rapidly increasing CO2 concentrations in the atmosphere over the last 200 years -- rising over 390 parts per million after remaining below 300 parts per million for the previous 800,000 years. And unlike natural heat variations, the current temperature increase caused by CO2 is being recorded occurring all around the globe – on the ground, in the air and in the oceans.\" - [Skeptical Science](http://www.skepticalscience.com/1500-year-natural-cycle.htm).\\n\\nAnd obviously, the 1500 year thing has made no headway in the scientific community.\\n\\n&gt;Though global warming is portrayed as causing oceans to rise, sea levels have been steadily rising at a rate of about seven inches per century for 5,000 years,\\n\\nOver the anthropogenic greenhouse era, [sea level rise is accelerating](http://4.bp.blogspot.com/-yZ6iFeOfuAs/UIocI8dngtI/AAAAAAAAAmc/LHzszkWuTJU/s1600/Hansen+figure+2.png).\\n\\n&gt;[...] and even that will reverse when the next Ice Age arrives in perhaps a few thousand years, at which time most of Europe and half of North America may be entombed in a mile-thick sheet of ice.\\n\\nThere are very few scientists who would agree with this speculation. \\n\\nCurrent estimates are that current CO2 levels are more than enough to delay the glaciation indefinitely.\\n\\n&gt; [No glacial inception is projected to occur at the current\\natmospheric CO2 concentrations of 390 ppmv \\\\(ref. 1\\\\). Indeed, model experiments suggest that in the current orbital configuration—which is characterized by a weak minimum in summer insolation—glacial inception would require CO2 concentrations below preindustrial levels of 280 ppmv (refs 2–4)](http://www.geology.ufl.edu/channell/images/PDFs%20for%20web%20site2/Tzedakis%20et%20al%202012.pdf)\\n\\n_\\n\\n&gt; Flooding is cited as a global warming risk, but most lowland floods are caused by seasonally melting snowcaps, with major storms as well as flooding a far greater risk during cooling periods than from seasonal icecaps.\\n\\nI don\\'t know if this is true, but it\\'s not relevant. Sea level is rising. The last time CO2 levels were this high, they were 25m higher. So we may be facing a future of unstable coastline, that will require a heavy cost in infrastructure protection or relocation for the foreseeable future.\\n\\n... edit: \\n\\n&gt; Species extinctions are cited as a consequence of warming, but the North America Pollen Database identifies nine continent-wide, temperature-driven reorganizations of vegetation corresponding to the 1,500-year cycle.\\n\\nThis one amazes me a bit.\\n\\nIt depends on his knowledge of current ecology.\\n\\nIf he doesn\\'t know there\\'s a mass extinction going on, I\\'d talk about how climate change in implicated as a cause in the recent drop in amphibian population biodiversity, that oceanic acidification threatens oceanic productivity, and coral bleaching damages the entire ocean ecosystem, as 25% of all oceanic species spend part of their life-cycle on reefs. [Giant red crabs are wiping out subantarctic ecosystems](http://www.newscientist.com/article/dn20876-giant-red-crabs-invade-the-antarctic-abyss.html#.UcwM_5zhdU8) that have been too cold for them these past 14 million years.\\n\\nIf he does know that, then assure him that with the combined pressures from pollution habitat loss and over-exploitation, plus the fact that land use change forms barriers to migrating to cooler climates (plus the fact that there may not be anywhere to move to, as for the first time in millions of years, [the climate is warming from near the top of an interglacial](http://www.daviesand.com/Choices/Precautionary_Planning/New_Data/IceCores1.gif) taking the climate into one that current species have never co-existed with), is exacerbating the biodiversity drop.\\n\\n&gt; The loss of crop land has been raised as a warming concern, but that has been largely discredited since temperatures would change little at the equator, and potentially arable acreage farther North would have less potential for cold, wet springs that inhibit planting.\\n\\nChanges in precipitation patterns have caused the famines of the 80s and 90s in the horn of Africa, the Sahel, and parts of South East Asia. The evidence that this is due to anthropogenic climate change is equivocal, but it is quite likely that AGW has killed millions already. ([An estimate of deaths by the A part of GW in 2000 by the WHO was 160,000 for that year alone](http://www.precaution.org/lib/05/warming_harms_health.051117.pdf)).\\n\\nCertainly *global* food production is expected to increase for the first 2°C of warming, and decline thereafter. But the land that opens up to agriculture will be over 80% in Russia or Canada, who aren\\'t starving, and the loss in land productivity because of droughts, floods, and erosion/salinity encroachment from sea level rise are in the poor parts of Africa, SE Asian and the sub-continent.\\n\\n&gt; In addition, modern farming methods have for over a century been discrediting Malthusian hand wringing.\\n\\nCertainly modernisation of the Horn of Africa, the Sahel, and Bangaldesh is an important aspect to adaptation. Education about and access to crop types that are tolerant to projected climate change (or salinity) for these people are also amongst the costs of adaptation. And while these are also development goals, and so not new goals because of climate change, they are not free merely because they have been desiderata for the past century or more. There is a cost to this development, and a cost to the increased urgency.\\n\\n... taking a minecraft break ... more still later....\\n\\n&gt; In the end, global warming skeptics such as the authors find that the historical record demonstrates that far more people and animals have died from prolonged cold than heat, which aside from exposure is attributable to the fact that cold causes people to huddle together, increasing the potential for disease.\\n\\nAgain I don\\'t know how verifiable this is or not, but it is not relevant to whether the warming is happening. And neither is past effect on human health of climate change a good predictor of future effect on human health, precisely because we are warming from near the peak warmth in the Quaternary period. The authors are not counting the regional climate changes that brings the mortality ... crop destruction and clean water compromisation by flooding, and crop destruction, and water insecurity from drought.\\n\\n&gt; There is also a chicken-and-egg debate as to whether CO2 is a lagging rather than causal factor in global warming, since CO2 concentrations actually appear to lag temperature change.\\n\\nThis is simply not true for the current warming. [CO2 levels dropped to their minimum in the 1600s, and were unmistakably rising by the second half of the 1800s](http://cdn.zmescience.com/wp-content/uploads/2013/03/co2-2.gif). [Temperature hit the minimum not until about 1910, and didn\\'t start the clearly anthropogenic rise until after 1950](http://ourchangingclimate.files.wordpress.com/2010/03/global_temp_yearly_2.png).\\n\\nIt is true that for most temperature rises in the ice core record, CO2 didn\\'t set off the warming, but participated in a positive feedback loop. The end of an glaciation takes about 6000 years, and the CO2 often (but not always) lags temperature by about 800 years. The current warming is qualitatively different, showing it is not like the end of an glaciation. (Which is not surprising because we are near the peak of an interglacial).\\n\\n&gt; Within the 1,500-year cycles there are long temperature oscillations that also appear to have little or no relation to human activity, the most recent series being a warming from (all years are rounded to centu- ries) 200 B.C. to A.D. 600 that encompassed the rise and fall of the Roman Empire, a cold period during the Dark Ages from 600 to 900 when the Roman and Mayan Empires collapsed due to rainfall deficits and reduced food output that caused internal and external pressure (to include invasion by barbarians displaced by changing crop patterns), a Medieval Warming or “Little Climate Optimum” from 900 to 1,300 that marked Europe’s emergence from the Dark Ages and the construction of Gothic architectural wonders, and a “Little Ice Age” from 1300 to 1850 when, during the coldest point of the 17th Century, Thomas Hobbes characterized life in the state of nature as \"solitary, poor, nasty, brutish, and short.\"\\n\\nWhile it is irrelevant to whether the current climate change is anthropogenic that there were past climate changes that weren\\'t, I don\\'t think that it is entirely correct to say that these particular temperature oscillations \" have little or no relation to human activity\". [The little ice age can be attributed to the black death.](http://news.bbc.co.uk/2/hi/science/nature/4755328.stm) *Europe\\'s \"Little Ice Age\" may have been triggered by the 14th Century Black Death plague, according to a new study.*\\n\\n&gt; There are many studies corroborating a Medieval Warming that was even warmer than our own era, which humanity thrived, and a Little Ice Age that produced as much disruption as the Dark Ages.\\n\\nI find this difficult to believe. I\\'d be interested in reading three published in the peer reviewed literature, that have more than half a dozen citations from scientists other than the authors.\\n\\nThere were small regions that have been warmer than current temperatures about that time (central Greenland is an example, I believe), but none so large as the northern hemisphere, and certainly not the globe.\\n\\nIn the meantime, [here](http://www.realclimate.org/images/m08.jpg) are a the most robust Northern Hemisphere temperature reconstructions from various proxy data prior that existed prior the 2007 IPCC report: Notice there are none that put the MWP as within half a degree of current temperatures.\\n\\n&gt; The current period, or Industrial Age, appears to us to be a natural warming that may restore the temperatures of the Medieval Warming, which is also known as the Medieval Climate Optimum. \\n\\nThis is not what it appears to be to people who have investigate whether or not the current warming is natural. [Here](http://journals.ametsoc.org/doi/pdf/10.1175/1520-0442%282004%29017%3C3721%3ACONAAF%3E2.0.CO%3B2) is one example. As you can see, natural warming accounts for none of the current warming since 1950. [Here](http://funnel.sfsu.edu/courses/gm310/articles/GlblWrming20thCenturyCauses.pdf) is an earlier one, with the same finding.\\n\\n&gt;  A possible explanation for long-term warming and cooling is variation in solar radiation.\\n\\nIt\\'s a *possible* explanation. It\\'s also irrelevant. [The current warming is certainly not due  to this mechanism.](http://www.skepticalscience.com/pics/1_GCRsvsTemps.jpg).\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment with VADER\n",
    "\n",
    "First, I thought about showing you also very naive way of computing sentiment - dictionary based but afterward I realised that there was no point in that. It was enough I told you about it cause it is more or less intuittive what happens.\n",
    "\n",
    "VADER is a slightly more complex approach that takes into account issues such as exclamation marks, negations and adjectival modifiers (i.e. words such as \"very\"). Its implementation is much more complex than what we did previously and we will not discuss it here. The good thing is that it is implemented in NLTK and extremely easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mikolaj/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import VADER, download its lexicon and initialize an instance of a sentiment analyzer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Import sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment1 = vader.polarity_scores(longest_comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.073, 'neu': 0.84, 'pos': 0.087, 'compound': 0.8432}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.107, 'neu': 0.796, 'pos': 0.097, 'compound': -0.0772},\n",
       " {'neg': 0.123, 'neu': 0.764, 'pos': 0.113, 'compound': -0.431}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment of sentences (more fine-grained picture)\n",
    "sent_sentiment1 = [ vader.polarity_scores(sent) for sent in sent_tokenize(longest_comment) ]\n",
    "sent_sentiment1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For instance, we can look at average compound scores over sentences\n",
    "sent_sentiment1_compound = sum(s['compound'] for s in sent_sentiment1) / len(sent_sentiment1)\n",
    "print(\"Average compound score over sentences (text I):\", sent_sentiment1_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data:\n",
    "    line['neg'] = vader.polarity_scores(line['body'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
