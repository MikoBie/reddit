{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "## Install modules\n",
    "!python -m spacy download pt_core_news_lg\n",
    "## Import modules\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"\n",
    "    A class that represents a corpus and has usefull methods defined.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, key=\"content\"):\n",
    "        \"\"\"\n",
    "        Reads from a JSON line file. Tokenizes and lemmatizes\n",
    "        the text under key. It writes out the new JSON line\n",
    "        file with a new field -- tokens.\n",
    "        Args:\n",
    "            path (str): a path to a JSON line.\n",
    "            key (str): a key with the content to lemmatize.\n",
    "        \"\"\"\n",
    "        self._path_original = path\n",
    "        self._key = key\n",
    "        self._dictionary = None\n",
    "        self._path = path.replace(\".\", \"_NLP.\")\n",
    "        with open(self._path, \"w\") as file:\n",
    "            n = 1\n",
    "            for line in open(self._path_original, \"r\"):\n",
    "                temp_dict = json.loads(line)\n",
    "                if temp_dict[self._key] == \"[deleted]\":\n",
    "                    continue\n",
    "                text_nlp = nlp(temp_dict[self._key])\n",
    "                temp_dict[\"tokens\"] = []\n",
    "                for token in text_nlp:\n",
    "                    is_stop = (\n",
    "                        token.is_stop\n",
    "                        or token.is_punct\n",
    "                        or token.is_space\n",
    "                        or token.is_bracket\n",
    "                        or token.is_currency\n",
    "                        or token.is_digit\n",
    "                        or token.is_quote\n",
    "                        or token.like_url\n",
    "                        or token.like_email\n",
    "                        or len(token) < 2\n",
    "                        or len(token) > 20\n",
    "                        or (not token.lemma_.isalpha())\n",
    "                    )\n",
    "                    if is_stop:\n",
    "                        continue\n",
    "                    else:\n",
    "                        temp_dict[\"tokens\"].append(token.lemma_.lower())\n",
    "                file.write(json.dumps(temp_dict) + \"\\n\")\n",
    "                sys.stdout.write(f\"\\rLine {n} processed\")\n",
    "                n += 1\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    def set_dictionary(self, dictionary):\n",
    "        \"\"\"\n",
    "        Assigns a gensim.corpora.dictionary.Dictioanry object\n",
    "        to self._dictionary.\n",
    "\n",
    "        Args:\n",
    "            dictionary (gensim.corpora.dictionary.Dictionary): a dictionary\n",
    "            that stores the frequencies of unique tokens in the corpus.\n",
    "        \"\"\"\n",
    "        self._dictionary = dictionary\n",
    "\n",
    "    def get_tokens(self):\n",
    "        \"\"\"\n",
    "        It takes the path to a JSON line file with comments from Reddit and\n",
    "        returns a generator that yields tokens for each comment.\n",
    "\n",
    "        Yields:\n",
    "            list : list of tokens for a comment from Reddit.\n",
    "        \"\"\"\n",
    "        for doc in open(self._path, \"r\"):\n",
    "            temp = json.loads(doc)\n",
    "            yield temp[\"tokens\"]\n",
    "\n",
    "    def get_bow(self):\n",
    "        \"\"\"\n",
    "        It takes a dictionary with frequencies of unique tokens in the corpus\n",
    "        and for each list of tokens returns a list of tuples that denote the\n",
    "        id of a given token and its frequency in a given document.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if the dictionary was not assigned to self._dictionary.\n",
    "\n",
    "        Yields:\n",
    "            list : a list of tuples that denote the id of a given token and its\n",
    "            frequency in a given document.\n",
    "        \"\"\"\n",
    "        if self._dictionary:\n",
    "            for doc in self.get_tokens():\n",
    "                yield self._dictionary.doc2bow(doc)\n",
    "        else:\n",
    "            raise ValueError(\"Dictionary has the value of None\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            list : a list of tuples that denote the id of a given token and\n",
    "            its frequency in a given document.\n",
    "        \"\"\"\n",
    "        for doc in self.get_bow():\n",
    "            yield doc\n",
    "\n",
    "    def get_topics(self, model):\n",
    "        \"\"\"\n",
    "        It takes a model and returns a generator that yields a mapping for each\n",
    "        comment from Reddit. Among other keys it returns the most probable topic\n",
    "        based on the LDA model provided and its probability.\n",
    "\n",
    "        Args:\n",
    "            model (gensim.models.ldamodel.LdaModel): Latent Dirchlet Allocation\n",
    "            model.\n",
    "\n",
    "        Yields:\n",
    "            dict : a mapping for each comment from Reddit. Among other keys it\n",
    "            returns the most prpobable topic based on the LDA model provided and\n",
    "            its probability.\n",
    "        \"\"\"\n",
    "        for doc in open(self._path, \"r\"):\n",
    "            temp = json.loads(doc)\n",
    "            topics = model.get_document_topics(self._dictionary.doc2bow(temp[\"tokens\"]))\n",
    "            topic, prob = sorted(topics, key=lambda x: x[1], reverse=True)[0]\n",
    "            temp[\"topic\"] = topic + 1\n",
    "            temp[\"topic_prob\"] = prob\n",
    "            yield temp\n",
    "\n",
    "\n",
    "class MyModel(LdaModel):\n",
    "    \"\"\"\n",
    "    Subclass of gensim.models.LdaModel.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_coherence(self, corpus):\n",
    "        \"\"\"\n",
    "        Returns the average coherence measure for the given model.\n",
    "\n",
    "        Args:\n",
    "            corpus (MyCorpus): A corpus on which the model is computed.\n",
    "\n",
    "        Returns:\n",
    "            float: the average coherence measure for the given model.\n",
    "        \"\"\"\n",
    "        top_topics = self.top_topics(corpus)\n",
    "        return sum([t[1] for t in top_topics]) / len(top_topics)\n",
    "\n",
    "    def get_top_tokens(self, corpus):\n",
    "        \"\"\"\n",
    "        Returns a list of dictionaries that depict the most probable\n",
    "        tokens for each topic.\n",
    "\n",
    "        Args:\n",
    "            corpus (MyCorpus): A corpus on which the model was computed.\n",
    "\n",
    "        Returns:\n",
    "            list: list of dicitionaries that depict the most probable\n",
    "            tokens fro each topic.\n",
    "        \"\"\"\n",
    "        top_tokens = self.top_topics(corpus)\n",
    "        return [{key: value for value, key in t[0]} for t in top_tokens]\n",
    "\n",
    "\n",
    "def run_lda_models(corpus, dictionary, min_topics, max_topics, step=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes a sequence of lda models for a given corpus and dictionary. It prints\n",
    "    the coherence measure and number of topics to the screen. It writes out the\n",
    "    model to disk.\n",
    "\n",
    "    Args:\n",
    "        corpus (MyModel): A stream of document vectors or sparse matrix of shape (num_documents, num_terms).\n",
    "        dictionary (dict): a mapping that assigns id to unique tokens from the corpus.\n",
    "        min_topics (int): the smallest number of topics to compute.\n",
    "        max_topics (int): the highest number of topics to compute.\n",
    "        step (int, optional): the size of the break inbetween computed models. Defaults to 1.\n",
    "    \"\"\"\n",
    "    name = input(\"Please provide the name of the model\\n\")\n",
    "    temp = dictionary[0]\n",
    "    id2word = dictionary.id2token\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    if not os.path.exists(\"png\"):\n",
    "        os.mkdir(\"png\")\n",
    "    for num_topic in range(min_topics, max_topics + 1, step):\n",
    "        model = MyModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            alpha=\"asymmetric\",\n",
    "            eta=\"auto\",\n",
    "            iterations=500,\n",
    "            passes=20,\n",
    "            eval_every=None,\n",
    "            num_topics=num_topic,\n",
    "            random_state=1044,\n",
    "            per_word_topics=True,\n",
    "        )\n",
    "        temp_dict = {}\n",
    "        temp_dict[\"name\"] = name\n",
    "        temp_dict[\"num_topics\"] = num_topic\n",
    "        temp_dict[\"coherence\"] = model.get_coherence(corpus=corpus)\n",
    "        path_name = os.path.join(\"models\", name + \"-\" + str(num_topic))\n",
    "        model.save(path_name)\n",
    "        print(temp_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEFORE YOU RUN the below\n",
    "\n",
    "Please make sure that you upload the files I sent you to the Google Colab. You need to press the folder icon on the left-hand side and press upload. Afterwards, please drag the file you uploaded to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read corpus\n",
    "corpus = MyCorpus(path=\"data/comments_portugal_veg.jl\", key=\"body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dictionary\n",
    "dictionary = Dictionary(corpus.get_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=0.9, no_above=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the dictionary to the corpus\n",
    "corpus.set_dictionary(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute modesl and write them out to the files\n",
    "run_lda_models(corpus=corpus, dictionary=dictionary, min_topics=2, max_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the module. It requires providing\n",
    "## the name of the model we want to load.\n",
    "model_name = input(\"Provide the name of the model you would like to load:\\r\")\n",
    "model_path = os.path.join(\"models\", model_name)\n",
    "model = LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out and write the figures with the most\n",
    "## probable tokens in each topic.\n",
    "list_top_tokens = model.get_top_tokens(corpus)\n",
    "for i in range(len(list_top_tokens)):\n",
    "    plt.barh(\n",
    "        list(list_top_tokens[i].keys()),\n",
    "        list(list_top_tokens[i].values()),\n",
    "        align=\"center\",\n",
    "    )\n",
    "    plt.xlim(0, 0.02)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Topic\" + \" \" + str(i + 1))\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.savefig(\"png/\" + \"topic\" + str(i + 1))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
