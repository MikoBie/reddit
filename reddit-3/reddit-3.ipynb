{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wegetarianizm -- Polska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import os\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "## stop_words = stopwords.words('polish')\n",
    "## Get the tokens to connect to Reddit Oficial API\n",
    "client_id = os.getenv(\"Reddit_Client_Id\")\n",
    "client_secret = os.getenv('Reddit_Client_Secret')\n",
    "password = os.getenv('Reddit_password')\n",
    "user_agent = os.getenv('Reddit_User_Agent')\n",
    "username = os.getenv('Reddit_Username')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret = client_secret,\n",
    "    password=password,\n",
    "    user_agent=user_agent,\n",
    "    username=username\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"\n",
    "    A class that represents a corpus and has usefull methods defined.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, key='content'):\n",
    "        \"\"\"\n",
    "        Reads from a JSON line file. Tokenizes and lemmatizes\n",
    "        the text under key. It writes out the new JSON line\n",
    "        file with a new field -- tokens.\n",
    "        Args:\n",
    "            path (str): a path to a JSON line.\n",
    "            key (str): a key with the content to lemmatize.\n",
    "        \"\"\"\n",
    "        self._path_original = path\n",
    "        self._key = key\n",
    "        self._dictionary = None\n",
    "        self._path = path.replace('.', '_NLP.')\n",
    "        with open(self._path, 'w') as file:\n",
    "            n = 1\n",
    "            for line in open(self._path_original, 'r'):\n",
    "                temp_dict = json.loads(line)\n",
    "                if temp_dict[self._key] == '[deleted]':\n",
    "                    continue\n",
    "                text_nlp = nlp(temp_dict[self._key])\n",
    "                temp_dict['tokens'] = []\n",
    "                for token in text_nlp:\n",
    "                    is_stop = token.is_stop or token.is_punct or token.is_space \\\n",
    "                        or token.is_bracket or token.is_currency or token.is_digit \\\n",
    "                        or token.is_quote or token.like_url or token.like_email \\\n",
    "                        or len(token) < 2 or len(token) > 20 or (not token.lemma_.isalpha())\n",
    "                    if is_stop:\n",
    "                        continue\n",
    "                    else:\n",
    "                        temp_dict['tokens'].append(token.lemma_.lower())\n",
    "                file.write( json.dumps(temp_dict) + '\\n')\n",
    "                sys.stdout.write(f'\\rLine {n} processed')\n",
    "                n += 1\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        \n",
    "    def set_dictionary(self, dictionary):\n",
    "        \"\"\"\n",
    "        Assigns a gensim.corpora.dictionary.Dictioanry object\n",
    "        to self._dictionary.\n",
    "\n",
    "        Args:\n",
    "            dictionary (gensim.corpora.dictionary.Dictionary): a dictionary\n",
    "            that stores the frequencies of unique tokens in the corpus.\n",
    "        \"\"\"\n",
    "        self._dictionary = dictionary\n",
    "\n",
    "    def get_tokens(self):\n",
    "        \"\"\"\n",
    "        It takes the path to a JSON line file with comments from Reddit and\n",
    "        returns a generator that yields tokens for each comment.\n",
    "\n",
    "        Yields:\n",
    "            list : list of tokens for a comment from Reddit. \n",
    "        \"\"\"\n",
    "        for doc in open(self._path, 'r'):\n",
    "            temp = json.loads(doc)\n",
    "            yield temp['tokens']\n",
    "    \n",
    "    def get_bow(self):\n",
    "        \"\"\"\n",
    "        It takes a dictionary with frequencies of unique tokens in the corpus\n",
    "        and for each list of tokens returns a list of tuples that denote the \n",
    "        id of a given token and its frequency in a given document.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if the dictionary was not assigned to self._dictionary.\n",
    "\n",
    "        Yields:\n",
    "            list : a list of tuples that denote the id of a given token and its\n",
    "            frequency in a given document.\n",
    "        \"\"\"\n",
    "        if self._dictionary:\n",
    "            for doc in self.get_tokens():\n",
    "                yield self._dictionary.doc2bow(doc)\n",
    "        else:\n",
    "            raise ValueError('Dictionary has the value of None')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            list : a list of tuples that denote the id of a given token and\n",
    "            its frequency in a given document.\n",
    "        \"\"\"\n",
    "        for doc in self.get_bow():\n",
    "            yield doc\n",
    "\n",
    "    def get_topics(self, model):\n",
    "        \"\"\"\n",
    "        It takes a model and returns a generator that yields a mapping for each\n",
    "        comment from Reddit. Among other keys it returns the most probable topic\n",
    "        based on the LDA model provided and its probability.\n",
    "\n",
    "        Args:\n",
    "            model (gensim.models.ldamodel.LdaModel): Latent Dirchlet Allocation\n",
    "            model.\n",
    "\n",
    "        Yields:\n",
    "            dict : a mapping for each comment from Reddit. Among other keys it\n",
    "            returns the most prpobable topic based on the LDA model provided and\n",
    "            its probability. \n",
    "        \"\"\"\n",
    "        for doc in open(self._path, 'r'):\n",
    "            temp = json.loads(doc)\n",
    "            topics = model.get_document_topics(self._dictionary.doc2bow(temp['tokens']))\n",
    "            topic, prob = sorted( topics, key = lambda x: x[1], reverse=True )[0]\n",
    "            temp['topic'] = topic + 1\n",
    "            temp['topic_prob'] = prob\n",
    "            yield temp\n",
    "\n",
    "                \n",
    "class MyModel(LdaModel):\n",
    "    \"\"\"\n",
    "    Subclass of gensim.models.LdaModel.\n",
    "    \"\"\"\n",
    "    def get_coherence(self, corpus):\n",
    "        \"\"\"\n",
    "        Returns the average coherence measure for the given model.\n",
    "\n",
    "        Args:\n",
    "            corpus (MyCorpus): A corpus on which the model is computed. \n",
    "\n",
    "        Returns:\n",
    "            float: the average coherence measure for the given model.\n",
    "        \"\"\"\n",
    "        top_topics = self.top_topics(corpus)\n",
    "        return sum([t[1] for t in top_topics]) / len(top_topics)\n",
    "    \n",
    "    def get_top_tokens(self, corpus):\n",
    "        \"\"\"\n",
    "        Returns a list of dictionaries that depict the most probable\n",
    "        tokens for each topic.\n",
    "\n",
    "        Args:\n",
    "            corpus (MyCorpus): A corpus on which the model was computed.\n",
    "\n",
    "        Returns:\n",
    "            list: list of dicitionaries that depict the most probable \n",
    "            tokens fro each topic.\n",
    "        \"\"\"\n",
    "        top_tokens = self.top_topics(corpus)\n",
    "        return [ { key : value for value, key in t[0] } for t in top_tokens ]\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "def run_lda_models(corpus, dictionary, min_topics, max_topics, step = 1, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes a sequence of lda models for a given corpus and dictionary. It prints\n",
    "    the coherence measure and number of topics to the screen. It writes out the\n",
    "    model to disk.\n",
    "\n",
    "    Args:\n",
    "        corpus (MyModel): A stream of document vectors or sparse matrix of shape (num_documents, num_terms).\n",
    "        dictionary (dict): a mapping that assigns id to unique tokens from the corpus.\n",
    "        min_topics (int): the smallest number of topics to compute.\n",
    "        max_topics (int): the highest number of topics to compute.\n",
    "        step (int, optional): the size of the break inbetween computed models. Defaults to 1.\n",
    "    \"\"\"\n",
    "    name = input(\"Please provide the name of the model\\n\")\n",
    "    temp = dictionary[0]\n",
    "    id2word = dictionary.id2token\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    if not os.path.exists('png'):\n",
    "        os.mkdir('png')\n",
    "    for num_topic in range(min_topics, max_topics+1, step):\n",
    "        model = MyModel( corpus = corpus,\n",
    "                         id2word=id2word,\n",
    "                         alpha = 'asymmetric',\n",
    "                         eta = 'auto',\n",
    "                         iterations = 500,\n",
    "                         passes = 20,\n",
    "                         eval_every=None,\n",
    "                         num_topics=num_topic,\n",
    "                         random_state=1044,\n",
    "                         per_word_topics=True)\n",
    "        temp_dict = {}\n",
    "        temp_dict['name'] = name\n",
    "        temp_dict['num_topics'] =  num_topic\n",
    "        temp_dict['coherence'] = model.get_coherence(corpus = corpus)\n",
    "        path_name = os.path.join('models', name + '-' + str(num_topic))\n",
    "        model.save(path_name) \n",
    "        print(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all submissions from Reddit Polska that contain a word \n",
    "## wegetarianizm\n",
    "subreddit = reddit.subreddit('Polska').search('wegetarianizm')\n",
    "## Create a list of dictionaries with the submissions\n",
    "submissions = [ { 'title' : line.title,\n",
    "                  'id' : line.id,\n",
    "                  'upvote_ratio' : line.upvote_ratio,\n",
    "                  'selftext' : line.selftext,\n",
    "                  'score' : line.score,\n",
    "                  'flair' : line.link_flair_text,\n",
    "                  'num_comments' : line.num_comments,\n",
    "                  'is_self' : line.is_self} \n",
    "               for line in subreddit ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information we can get from a submissions. The following fields are out there but probably we don't need all of them. I put them here just in case.\n",
    "\n",
    "* author -- provides an instance of Redditor.\n",
    "* author_flair_text -- the text content of the author’s flair, or None if not flaired. In simple terms, a flair on reddit is a kind of tag added to either post or username. They are meant to categorize posts or users.\n",
    "* clicked -- whether or not the submission has been clicked by the client.\n",
    "* comments -- provides an instance of CommentForest.\n",
    "* created_utc -- time the submission was created, represented in Unix Time.\n",
    "* distinguished -- whether or not the submission is distinguished.\n",
    "* edited -- Whether or not the submission has been edited.\n",
    "* id -- ID of the submission.\n",
    "* is_original_content -- whether or not the submission has been set as original content.\n",
    "* is_self -- whether or not the submission is a selfpost (text-only).\n",
    "* link_flair_template_id -- the link flair’s ID.\n",
    "* link_flair_text -- The link flair’s text content, or None if not flaired.\n",
    "* locked -- whether or not the submission has been locked.\n",
    "* name -- Fullname of the submission.\n",
    "* num_comments -- the number of comments on the submission.\n",
    "* over_18 -- whether or not the submission has been marked as NSFW.\n",
    "* permalink -- a permalink for the submission.\n",
    "* poll_data -- a PollData object representing the data of this submission, if it is a poll submission.\n",
    "* saved -- whether or not the submission is saved.\n",
    "* score -- the number of upvotes for the submission.\n",
    "* selftext -- the submissions’ selftext - an empty string if a link post.\n",
    "* spoiler -- whether or not the submission has been marked as a spoiler.\n",
    "* stickied -- whether or not the submission is stickied.\n",
    "* subreddit -- provides an instance of Subreddit.\n",
    "* title -- the title of the submission.\n",
    "* upvote_ratio -- the percentage of upvotes from all votes on the submission.\n",
    "* url -- the URL the submission links to, or the permalink if a selfpost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just print out the most important information about each submission.\n",
    "for sub in submissions: print({ 'id' : sub['id'], 'title' : sub['title'], 'num_comments' : sub['num_comments'] })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information on a single comment. The following fields are otu there but probably we don't need all of them. I put them here just in case.jjj\n",
    "\n",
    "* author -- provides an instance of Redditor.\n",
    "* body -- the body of the comment, as Markdown.\n",
    "* body_html -- he body of the comment, as HTML.\n",
    "* created_utc -- time the comment was created, represented in Unix Time.\n",
    "* distinguished -- whether or not the comment is distinguished.\n",
    "* edited -- whether or not the comment has been edited.\n",
    "* id -- the ID of the comment.\n",
    "* is_submitter -- whether or not the comment author is also the author of the submission.\n",
    "* link_id -- the submission ID that the comment belongs to.\n",
    "* parent_id -- he ID of the parent comment (prefixed with t1_). If it is a top-level comment, this returns the submission ID instead (prefixed with t3_).\n",
    "* permalink -- a permalink for the comment. Comment objects from the inbox have a context attribute instead.\n",
    "* replies -- provides an instance of CommentForest.\n",
    "* saved -- whether or not the comment is saved.\n",
    "* score -- the number of upvotes for the comment.\n",
    "* stickied -- whether or not the comment is stickied.\n",
    "* submission -- provides an instance of Submission. The submission that the comment belongs to.\n",
    "* subreddit -- provides an instance of Subreddit. The subreddit that the comment belongs to.\n",
    "* subreddit_id -- the subreddit ID that the comment belongs to.\n",
    "\n",
    "And for the Redditor\n",
    "\n",
    "* comment_karma -- the comment karma for the Redditor.\n",
    "* comments -- provide an instance of SubListing for comment access.\n",
    "* submissions -- provide an instance of SubListing for submission access.\n",
    "* created_utc -- time the account was created, represented in Unix Time.\n",
    "* has_verified_email -- whether or not the Redditor has verified their email.\n",
    "* icon_img -- the url of the Redditors’ avatar.\n",
    "* id -- the ID of the Redditor.\n",
    "* is_employee -- whether or not the Redditor is a Reddit employee.\n",
    "* is_friend -- whether or not the Redditor is friends with the authenticated user.\n",
    "* is_mod -- whether or not the Redditor mods any subreddits.\n",
    "* is_gold -- whether or not the Redditor has active Reddit Premium status.\n",
    "* is_suspended -- whether or not the Redditor is currently suspended.\n",
    "* link_karma -- the link karma for the Redditor.\n",
    "* name -- the Redditor’s username.\n",
    "* subreddit -- if the Redditor has created a user-subreddit, provides a dictionary of additional attributes. See below.\n",
    "* subreddit[\"banner_img\"] -- the URL of the user-subreddit banner.\n",
    "* subreddit[\"name\"]-- the fullname of the user-subreddit.\n",
    "* subreddit[\"over_18\"] -- whether or not the user-subreddit is NSFW.\n",
    "* subreddit[\"public_description\"] -- the public description of the user-subreddit.\n",
    "* subreddit[\"subscribers\"] -- the number of users subscribed to the user-subreddit.\n",
    "* subreddit[\"title\"] -- the title of the user-subreddit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/portugal/comments/uap723/somos_o_pa%C3%ADs_com_menos_vegetarianos_per_capita/\n",
    "\n",
    "https://www.reddit.com/r/PORTUGALCARALHO/comments/xhuqbq/consumo_de_carne_na_europa/\n",
    "\n",
    "Other links:\n",
    "\n",
    "https://www.reddit.com/r/portugal/comments/rprwkj/é_poss%C3%ADvel_ter_uma_dieta_vegan_saudável_serio/?utm_source=share&utm_medium=web2x&context=3\n",
    "\n",
    "https://www.reddit.com/r/portugal/comments/4oj1mg/transição_para_veganvegetariano/?utm_source=share&utm_medium=web2x&context=3\n",
    "\n",
    "https://www.reddit.com/r/portugal/comments/lrcixp/qual_a_vossa_opinião_em_relação_ao_veganismo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select a submission by id -- this one is about vegetarianism and veganism\n",
    "## on the Reddit Polska.\n",
    "submission = reddit.submission(\"lrcixp\")\n",
    "\n",
    "## Set the option to get all the comments\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "## Iterate over all the comments. Ignore the comments\n",
    "## tree. Write the comments to the JSON line file.\n",
    "with open('data/comments_portugal_healthy_diet.jl', 'w') as file:\n",
    "  for comment in submission.comments.list():\n",
    "      temp_dict = {}\n",
    "      temp_dict['body'] = comment.body\n",
    "      temp_dict['score'] = comment.score\n",
    "      temp_dict['link'] = comment.permalink\n",
    "      temp_dict['parent_id'] = comment.parent_id\n",
    "      try:\n",
    "          temp_dict['author'] = { 'name' : comment.author.name,\n",
    "                                  'karma' : comment.author.comment_karma,\n",
    "                                  'created_utc' : datetime.fromtimestamp(comment.author.created_utc).strftime('%d-%m-%Y %H:%M:%S'),\n",
    "                                  'has_verified_email' : comment.author.has_verified_email,\n",
    "                                  #'is_suspended' : comment.author.is_suspended,\n",
    "                                  'is_gold' : comment.author.is_gold\n",
    "          }\n",
    "      except:\n",
    "          pass\n",
    "      temp_dict['created_utc'] = datetime.fromtimestamp(comment.created_utc).strftime('%d-%m-%Y %H:%M:%S')\n",
    "      temp_dict['edited'] = comment.edited\n",
    "      temp_dict['is_submitter'] = comment.is_submitter\n",
    "      \n",
    "      file.write(json.dumps(temp_dict) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read corpus\n",
    "corpus = MyCorpus(path = 'data/comments_portugal_healthy_diet.jl', key = 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dictionary\n",
    "dictionary = Dictionary( corpus.get_tokens() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=.9, no_above=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the dictionary to the corpus\n",
    "corpus.set_dictionary(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute modesl and write them out to the files\n",
    "run_lda_models(corpus = corpus, dictionary = dictionary, min_topics=2, max_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the module. It requires providing\n",
    "## the name of the model we want to load.\n",
    "model_name = input('Provide the name of the model you would like to load:\\r')\n",
    "model_path = os.path.join('models', model_name)\n",
    "model = LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out and write the figures with the most \n",
    "## probable tokens in each topic.\n",
    "list_top_tokens = model.get_top_tokens(corpus)\n",
    "for i in range(len(list_top_tokens)):\n",
    "    plt.barh(list(list_top_tokens[i].keys()), list(list_top_tokens[i].values()), align = 'center')\n",
    "    plt.xlim(0,.02)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title('Topic' + ' ' + str(i + 1))\n",
    "    plt.xlabel('Probability')\n",
    "    plt.savefig('png/' + 'topic' + str(i + 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write out the results into a CSV file\n",
    "pd.DataFrame.from_records(line for line in corpus.get_topics(model = model)).to_excel('data/' + model_name + 'topics.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d55d0eb9076beff1aad1bfd9ab8f276d192cdf8ff5403c32a6f6618edbf4e356"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
